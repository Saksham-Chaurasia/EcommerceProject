---------Local-->HDFS-->mysql-->hive---------------
---------1st outcome --------------------- Data migration---------
-------mysql processing ---------------------


1. Copy from local to hdfs

* hadoop fs -put companyProject   --> it will directly make a comapny to hdfs location 'hadoop fs -ls'

* hadoop fs -ls

validating--

* hadoop fs -cat companyProject/purchase_data.csv 

* mysql -utraining -ptraining   or mysql --user=training --password=training  (in a new terminal)

* create database commerce;

* use commerce;

* create table purchase(userID varchar(20) not null primary key, timestamp varchar(20), amount int);

* describe purchase; --> just validating

--- encountering an error -----

* sqoop export --connect jdbc:mysql://localhost/commerce --username training --password training --table purchase --export-dir companyProject/purchase_data.csv --input-optionally-enclosed-by '\"' -m4

--> using --input-optionally-enclosed-by will resolve the issue of double quotes problem in our data.
--> while using this because our data has header file, and also our data has enclosed double quotes at the start and end. 
--> to resolve this -- approach -- to load the data in staging table first from there i'm going to load the data into my final table..

-------------------------------------------------------------

-------creating staging table-----

* create table stage_purchase(userID varchar(20) not null primary key, timestamp varchar(20), amount varchar(20));

* sqoop export --connect jdbc:mysql://localhost/commerce --username training --password training --table stage_purchase --export-dir companyProject/purchase_data.csv --input-escaped-by '\"' -m4

--> --input-escaped-by '\"' will remove the double quotes at the start and end of the table

--> it loaded all the data into my staging table 

--> now the problem is if i want to use sqoop eval to load the data from one table to another mysql table, the datatype should be same. now in my case timestamp is going to be create a problem. 


------- now create original table ------
* create table purchase(userID varchar(20) not null primary key, timestamp varchar(20), amount int);

----- using sqoop eval to load the data and convert into our datatype also ------

* sqoop eval --connect jdbc:mysql://localhost/commerce --username training --password training --query "insert into purchase select * from stage_purchase where userID !='userID';"



------- Similary for other two datasets we need to do --------

--- dataset customer---

* create table stage_customer(userID varchar(20) not null primary key, name varchar(30), email varchar(30));

* create table customer(userID varchar(20) not null primary key, name varchar(30), email varchar(30));

* sqoop export --connect jdbc:mysql://localhost/commerce --username training --password training --table stage_customer --export-dir companyProject/customer_data.csv --input-escaped-by '\"' -m4

* sqoop eval --connect jdbc:mysql://localhost/commerce --username training --password training --query "insert into customer select * from stage_customer where userID !='userID';"

------------------------------------------------


---- dataset clickstream ---

* create table stage_clickstream(userID varchar(20), timestamp varchar(20), page varchar(30));

* create table clickstream(userID varchar(20), timestamp timestamp, page varchar(30));

* sqoop export --connect jdbc:mysql://localhost/commerce --username training --password training --table stage_clickstream --export-dir companyProject/clickstream_data.csv --input-escaped-by '\"' -m4

* sqoop eval --connect jdbc:mysql://localhost/commerce --username training --password training --query "insert into clickstream select * from stage_clickstream where userID !='userID';"

------------------------------------------------------------------------------


-------mysql to hive ------
* create database commerce;
* use commerce;
* set hive.cli.print.current.db=true;

* sqoop import --connect jdbc:mysql://localhost/commerce --username training --password training --table purchase --hive-table commerce.purchase --create-hive-table --hive-import -m1

* sqoop import --connect jdbc:mysql://localhost/commerce --username training --password training --table customer --hive-table commerce.customer --create-hive-table --hive-import -m1

* sqoop import --connect jdbc:mysql://localhost/commerce --username training --password training --table clickstream --hive-table commerce.clickstream --create-hive-table --hive-import -m1

----------------------------------------------------------------------------------

sqoop import --connect jdbc:mysql://localhost/commerce --username saksham --password password --table purchase --hive-table commerce.purchase --create-hive-table --hive-import -m1

sqoop import --connect jdbc:mysql://localhost/commerce --username saksham --password password --table customer --hive-table commerce.customer --create-hive-table --hive-import -m1

sqoop import --connect jdbc:mysql://localhost/commerce --username saksham --password password --table clickstream --hive-table commerce.clickstream --create-hive-table --hive-import -m1
